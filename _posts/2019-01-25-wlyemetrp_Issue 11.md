---
layout: post
title: "Econometrica Vol 86 (2018)Issue 11"
description: ""
category: 经济学
tags: [期刊]
---
{% include JB/setup %}

<p><strong>Nonparametric Analysis of Random Utility Models</strong></p>
<p>Yuichi Kitamura&nbsp;&nbsp;&nbsp;Jörg Stoye&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;This paper develops and implements a nonparametric test of random utility models. The motivating application is to test the null hypothesis that a sample of cross‐sectional demand distributions was generated by a population of rational consumers. We test a necessary and sufficient condition for this that does not restrict unobserved heterogeneity or the number of goods. We also propose and implement a control function approach to account for endogenous expenditure. An econometric result of independent interest is a test for linear inequality constraints when these are represented as the vertices of a polyhedral cone rather than its faces. An empirical application to the U.K. Household Expenditure Survey illustrates computational feasibility of the method in demand problems with five goods.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>The Sorted Effects Method: Discovering Heterogeneous Effects Beyond Their Averages</strong></p>
<p>Victor Chernozhukov&nbsp;&nbsp;&nbsp;Iván Fernández‐Val&nbsp;&nbsp;&nbsp;Ye Luo&nbsp;&nbsp;&nbsp;Ivan Fernandez-Val&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;The partial (ceteris paribus) effects of interest in nonlinear and interactive linear models are heterogeneous as they can vary dramatically with the underlying observed or unobserved covariates. Despite the apparent importance of heterogeneity, a common practice in modern empirical work is to largely ignore it by reporting average partial effects (or, at best, average effects for some groups). While average effects provide very convenient scalar summaries of typical effects, by definition they fail to reflect the entire variety of the heterogeneous effects. In order to discover these effects much more fully, we propose to estimate and report sorted effects—a collection of estimated partial effects sorted in increasing order and indexed by percentiles. By construction, the sorted effect curves completely represent and help visualize the range of the heterogeneous effects in one plot. They are as convenient and easy to report in practice as the conventional average partial effects. They also serve as a basis for classification analysis, where we divide the observational units into most or least affected groups and summarize their characteristics. We provide a quantification of uncertainty (standard errors and confidence bands) for the estimated sorted effects and related classification analysis, and provide confidence sets for the most and least affected groups. The derived statistical results rely on establishing key, new mathematical results on Hadamard differentiability of a multivariate sorting operator and a related classification operator, which are of independent interest. We apply the sorted effects method and classification analysis to demonstrate several striking patterns in the gender wage gap. We find that this gap is particularly strong for married women, ranging from −60% to 0% between the 2% and 98% percentiles, as a function of observed and unobserved characteristics; while the gap for never married women ranges from −40% to +20%. The most adversely affected women tend to be married, do not have college degrees, work in sales, and have high levels of potential experience.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Identifying Effects of Multivalued Treatments</strong></p>
<p>Sokbae Lee&nbsp;&nbsp;&nbsp;Bernard Salanié&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;Multivalued treatment models have typically been studied under restrictive assumptions: ordered choice, and more recently, unordered monotonicity. We show how treatment effects can be identified in a more general class of models that allows for multidimensional unobserved heterogeneity. Our results rely on two main assumptions: treatment assignment must be a measurable function of threshold‐crossing rules, and enough continuous instruments must be available. We illustrate our approach for several classes of models.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Monte Carlo Confidence Sets for Identified Sets</strong></p>
<p>Xiaohong Chen&nbsp;&nbsp;&nbsp;Timothy M. Christensen&nbsp;&nbsp;&nbsp;Elie Tamer&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;It is generally difficult to know whether the parameters in nonlinear econometric models are point‐identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of the full parameter vector and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. The CSs are based on level sets of “optimal” criterion functions (such as likelihoods, optimally‐weighted or continuously‐updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations from the quasi‐posterior distribution of the criterion. We establish new Bernstein–von Mises (or Bayesian Wilks) type theorems for the quasi‐posterior distributions of the quasi‐likelihood ratio (QLR) and profile QLR in partially‐identified models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially‐identified regular models, and have valid but potentially conservative coverage in models whose local tangent spaces are convex cones. Further, our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We provide local power properties and uniform validity of our CSs over classes of DGPs that include point‐ and partially‐identified models. Finally, we present two simulation experiments and two empirical examples: an airline entry game and a model of trade flows.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Surprised by the Hot Hand Fallacy? A Truth in the Law of Small Numbers</strong></p>
<p>Joshua B. Miller&nbsp;&nbsp;&nbsp;Adam Sanjurjo&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;We prove that a subtle but substantial bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. The magnitude of this streak selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work. We observe that the canonical study in the influential hot hand fallacy literature, along with replications, are vulnerable to the bias. Upon correcting for the bias, we find that the longstanding conclusions of the canonical study are reversed.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Aggregating Elasticities: Intensive and Extensive Margins of Women's Labor Supply</strong></p>
<p>Orazio Attanasio&nbsp;&nbsp;&nbsp;Peter Levell&nbsp;&nbsp;&nbsp;Hamish Low&nbsp;&nbsp;&nbsp;Virginia Sánchez‐Marcos&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;We show that there is substantial heterogeneity in women's labor supply elasticities at the micro level and highlight the implications for aggregate behavior. We consider both intertemporal and intratemporal choices, and identify intensive and extensive responses in a consistent life‐cycle framework, using US CEX data. Heterogeneity is due to observables, such as age, wealth, hours worked, and the wage level, as well as to unobservable tastes for leisure: the median Marshallian elasticity for hours worked is 0.18, with corresponding Hicksian elasticity of 0.54 and Frisch elasticity of 0.87. At the 90th percentile, these values are 0.79, 1.16, and 1.92. Responses at the extensive margin explain about 54% of the total labor supply response for women under 30, although this declines with age. Aggregate elasticities are higher in recessions, and increase with the length of the recession. The heterogeneity at the micro level means that the aggregate labor supply elasticity is not a structural parameter: any aggregate elasticity will depend on the demographic structure of the economy as well as the distribution of wealth and the particular point in the business cycle.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>The Historical State, Local Collective Action, and Economic Development in Vietnam</strong></p>
<p>Melissa Dell&nbsp;&nbsp;&nbsp;Nathaniel Lane&nbsp;&nbsp;&nbsp;Pablo Querubin&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;This study examines how the historical state conditions long‐run development, using Vietnam as a laboratory. Northern Vietnam (Dai Viet) was ruled by a strong, centralized state in which the village was the fundamental administrative unit. Southern Vietnam was a peripheral tributary of the Khmer (Cambodian) Empire, which followed a patron‐client model with more informal, personalized power relations and no village intermediation. Using a regression discontinuity design, the study shows that areas exposed to Dai Viet administrative institutions for a longer period prior to French colonization have experienced better economic outcomes over the past 150 years. Rich historical data document that in Dai Viet villages, citizens have been better able to organize for public goods and redistribution through civil society and local government. We argue that institutionalized village governance crowded in local cooperation and that these norms persisted long after the original institutions disappeared.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>The Value of Regulatory Discretion: Estimates From Environmental Inspections in India</strong></p>
<p>Esther Duflo&nbsp;&nbsp;&nbsp;Michael Greenstone&nbsp;&nbsp;&nbsp;Rohini Pande&nbsp;&nbsp;&nbsp;Nicholas Ryan&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;High pollution persists in many developing countries despite strict environmental rules. We use a field experiment and a structural model to study how plant emission standards are enforced. In collaboration with an Indian environmental regulator, we experimentally doubled the rate of inspection for treatment plants and required that the extra inspections be assigned randomly. We find that treatment plants only slightly increased compliance. We hypothesize that this weak effect is due to poor targeting, since the random inspections in the treatment found fewer extreme violators than the regulator's own discretionary inspections. To unbundle the roles of extra inspections and the removal of discretion over what plants to target, we set out a model of environmental regulation where the regulator targets inspections, based on a signal of pollution, to maximize plant abatement. Using the experiment to identify key parameters of the model, we find that the regulator aggressively targets its discretionary inspections, to the degree that half of the plants receive fewer than one inspection per year, while plants expected to be the dirtiest may receive ten. Counterfactual simulations show that discretion in targeting helps enforcement: inspections that the regulator assigns cause three times more abatement than would the same number of randomly assigned inspections. Nonetheless, we find that the regulator's information on plant pollution is poor, and improvements in monitoring would reduce emissions.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Provider Incentives and Healthcare Costs: Evidence From Long‐Term Care Hospitals</strong></p>
<p>Liran Einav&nbsp;&nbsp;&nbsp;Amy Finkelstein&nbsp;&nbsp;&nbsp;Neale Mahoney&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;We study the design of provider incentives in the post‐acute care setting—a high‐stakes but under‐studied segment of the healthcare system. We focus on long‐term care hospitals (LTCHs) and the large (approximately $13,500) jump in Medicare payments they receive when a patient's stay reaches a threshold number of days. Discharges increase substantially after the threshold, with the marginal discharged patient in relatively better health. Despite the large financial incentives and behavioral response in a high mortality population, we are unable to detect any compelling evidence of an impact on patient mortality. To assess provider behavior under counterfactual payment schedules, we estimate a simple dynamic discrete choice model of LTCH discharge decisions. When we conservatively limit ourselves to alternative contracts that hold the LTCH harmless, we find that an alternative contract can generate Medicare savings of about $2,100 per admission, or about 5% of total payments. More aggressive payment reforms can generate substantially greater savings, but the accompanying reduction in LTCH profits has potential out‐of‐sample consequences. Our results highlight how improved financial incentives may be able to reduce healthcare spending, without negative consequences for industry profits or patient health.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  

<p><strong>Long Memory via Networking</strong></p>
<p>Susanne Schennach&nbsp;&nbsp;&nbsp;</p>
<p><strong>Abstract:</strong>&nbsp;&nbsp;&nbsp;Many time series exhibit “long memory”: Their autocorrelation function decays slowly with lag. This behavior has traditionally been modeled via unit roots or fractional Brownian motion and explained via aggregation of heterogeneous processes, nonlinearity, learning dynamics, regime switching, or structural breaks. This paper identifies a different and complementary mechanism for long‐memory generation by showing that it can naturally arise when a large number of simple linear homogeneous economic subsystems with short memory are interconnected to form a network such that the outputs of the subsystems are fed into the inputs of others. This networking picture yields a type of aggregation that is not merely additive, resulting in a collective behavior that is richer than that of individual subsystems. Interestingly, the long‐memory behavior is found to be almost entirely determined by the geometry of the network, while being relatively insensitive to the specific behavior of individual agents.</p>
<p><strong>DOI</strong>:
</p>
<p> </p>
<p> </p>
  